\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}

\title{Fast Fourier Transform -- Analyzing and Modifying Digital Audio Frequency Content}
\author{A.J. Rideout}
\date{January 2026}

\begin{document}

\maketitle

\section{Declaration of Use of Generative AI:} Generative language models have been used to debug and refine hand-written python code used within this project, as well as to quickly retrieve information from documentation of used libraries. 

\section{Abstract} Fast Fourier transform algorithms are the standard way to calculate the discrete Fourier transform in most practical applications. In the case of digital audio, the Fourier transform is especially valuable for analysis and modification of audio signals. It allows us to not only characterize signals, but shape their sound through modification of frequency content. This is known as filtering, and it is one of the most fundamental methods of altering sound, and it can be accomplished in many ways. After developing a fast Fourier transform algorithm, we develop and refine a simple frequency-domain filter. We first created an ideal filter, which has drawbacks through the Gibbs effect. We used a window function, the Hanning window, to combat the ringing and overshoot known as the Gibbs effect, creating a more usable, stable filter.

\section{Introduction} The Fourier transform is intuitively valuable when analyzing audio signals, as breaking down a more complex town into a series of frequencies that compose it allows one to intuitively understand the fundamental building blocks that construct a sound and contribute to its character. In most cases however, calculating the Fourier transform can be costly and time consuming -- especially as the signal reaches larger sizes. Luckily, there exist alternative means/routes to calculating the Discrete Fourier Transform (DFT) for a dataset, which lead to alternative results. These often exploit symmetries or properties of the signal to introduce "shortcuts" that allow reduced operations to calculate the transform. For example, the Cooley-Tukey algorithm operates exclusively on data sets of size $2^m$, and takes advantage of symmetries in the exponential terms to introduce a recursive "divide-and-conquer" approach that breaks the larger DFT into a series of smaller problems that in all significantly reduce the workload required. \cite{1} Once a Fourier Transform is obtained, for observations sake we really only use the first $N/2$ coefficients, as for real-valued functions (which audio signals tend to be!) the Fourier transform will have a conjugate symmetry. \cite{2} However, we do still need the data after that if we plan on using the Inverse Fourier transform on the DFT data to obtain our original signal. As well, the data output by our FFT is not immediately in Hz. We must also assign frequencies in Hz. to the "bins" (each discrete frequency on our frequency domain) output by our DFT, which have frequency values determined by the sample rate and amount of samples of our audio. The frequency of the $k$th bin is given by $k\frac{f_s}{N}$, where $f_s$ is our sample rate, and $N$ is the amount of samples. \cite{3}
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{Waveform Vs. DFT.png}
    \caption{A comparison of the waveform and DFT output of a clean guitar sample.}
    \label{fig:placeholder}
\end{figure}
 Being able to return to our original signal is important if we plan on making changes in the frequency-domain DFT data. Any changes we make within the frequency-domain will be present in the output of the inverse FFT. This effectively allows us to shape our sounds by controlling the presence of individual frequencies. This is known as filtering, and there exist many ways of doing so, but altering the frequency domain, while not conducive to modern workflow standards, is a great demonstration of filtering as a concept and the control that obtaining the DFT of our audio can grant us. We developed two simple filter types: ideal, and Hanning. The ideal filters are not ideal in the sense of practicality, but ideal in terms of having perfect frequency response. However, this perfect frequency response creates discontinuities in the frequency domain, which can result in overshoots caused by an infinite impulse response in the time domain. (leakage!) This results in an audible ringing sound, especially in signals with frequent and/or drastic changes in amplitude. \cite{4} This can be combated by using a window function, which we can apply to essentially interpolate the drop-off of our filter. The Hanning window function is a common choice for this task. \cite{5} Using this, we can create a bell-shaped mask that we can multiply over our frequency domain, as opposed to flattening our signal's frequency content abruptly. This results in a less drastic cutoff (less frequency response) but minimizes the Gibbs effect ringing. 
 
\section{Method}  The Fourier transform is an integral transform that decomposes a function over a continuous domain into a spectrum of complex exponential elements, each corresponding to some frequency that is present within the original data. For each element, there exists a coefficient that denotes that frequencies presence within the signal, with a higher value indicating an abundance of that frequency. The $n$th Fourier coefficient, $\hat{f}(n)$, is defined as: \[\hat{f}(n)=\int_{\mathbb{T}^1}f(x)e^{-2\pi inx}\space dx\]
where $\mathbb{T}^1$ defines the one dimensional torus: $\mathbb{R}/\mathbb{Z}$. \cite{1}. The Fourier transform is similarly defined: \[\hat{f}(\xi)=\int_{\mathbb{R}}f(x)e^{-2\pi ix\xi}\space dx\] These definitions are derived from the inner product of a space created over $\mathbb{T}^1$, where each exponential $e^{-2\pi inx}$ (which we may denote as $e_n$) is orthogonal to each other \cite{1} and refers to a specific amount of rotations about $\mathbb{T}$. By taking the inner product of our function $f(x)$ with $e_n$, we are essentially finding a projection of our function onto the amount of rotations corresponding to $e_n$, which is larger in magnitude when it corresponds highly to $e_n$. 
When working with digital audio signals, an important thing to keep in mind is that audio signals are discrete. They consist of a series of samples that represent the amplitude of our sound at a given time. The rate at which samples are taken in and played back at is known as the sample rate. While initially sample rate may seem arbitrary, it is important for time-based signals that the sample rate properly accommodates our data without using up too much space. The Nyquist-Shannon sampling theorem states that in order for a signal to be reproduced perfectly digitally, it must have a sample rate that is at least double the highest relevant frequency of study. \cite{6} In the case of audio, a common standard is 44100 Hz., as that well accommodates the human range of hearing, with most people struggling with frequencies above 20000 Hz.. Because we will be working with discrete data, we cannot use the main definition of the Fourier transform. Luckily, a lot of the concepts behind the Fourier transform carry well into discrete domains. The $n$th Fourier coefficient for a discrete domain function/dataset is defined as: \[\hat{f}(n)=\frac{1}{\sqrt{N}}\sum_{k=0}^{N-1}f(k)e^{{-2\pi ikn}/N}\] \cite{1} Where $N$ is the total number of samples in our data set. Note that the $\frac{1}{\sqrt{N}}$ factor is not necessary for application on digital audio, especially if we are normalizing our audio (scaling it to the highest amplitude it contains) before and after our transform. The concepts behind translating over to a discrete domain are similar -- $e_n=e^{{-2\pi ikn}/N}$ in this case is an $N$th root of unity, which similarly can communicate a complex number having completed $n$ rotations about a unit circle. Thus, in taking a discrete inner product between $f$ and $e_n$ we are similarly probing for correlation between $f$ and the frequency $e_n$ denotes, where $f$ having a stronger match to the oscillation of $e_n$ indicates a higher coefficient at $n$. We can go from Fourier coefficients back to $f$ by the inverse discrete Fourier transform: \[f=\sum^{N-1}_{n=0}\hat{f}(n)e^{{2\pi ikn}/N}\] \cite{1} noting that the sign of the exponent is flipped.
With this definition of the DFT, we can retrieve the coefficients of our sound data, analyze or modify them, and convert them back into our original signal. However, doing so, especially for large $N$, is computationally expensive and often not calculated according to this definition. Instead, there exist widely-used fast Fourier transform (FFT) algorithms that use alternate, less intensive calculations that yield the same result as the DFT. Many take advantage of existing symmetries of the DFT for signals with specific characteristics. One widely used example of an FFT algorithm is the Cooley-Tukey (Radix-2) algorithm, which operates specifically on data sets of size $N=2^m$. The algorithm reduces the required operations from $O(N^2)$ operations to $O(N log_2N)$ operations. \cite{1} Essentially, the algorithm starts by splitting our original sample into two separate chunks -- samples of even and odd indices, which can be repeated recursively until we are working with many DFTs of size 2, since $N=2^m$. When substituted into the DFT formula, it yields: \[\hat{f}(n)=\frac{1}{\sqrt{N}}\left[\sum^{N/2-1}_{m=0}f(2m)\omega_N^{n(2m)}+\sum_{m=0}^{N/2-1}f(2m+1)\omega_N^{n(2m+1)}\right]\] \[=\frac{1}{\sqrt{N}}\left[\sum^{N/2-1}_{m=0}f(2m)\omega_{N/2}^{n \cdot m}+\omega_{N}^{n}\sum_{m=0}^{N/2-1}f(2m+1)\omega_{N/2}^{n \cdot m}\right]\]\cite{1} Again, note that for practical use cases the $\frac{1}{\sqrt{N}}$ factor can be omitted. 
To meet the $N=2^m$ requirement of the Cooley-Tukey algorithm, we may pad our signal with zeroes (end silence) to reach the nearest power of 2. This comes with the small, but welcome benefit of reducing our bin size (increasing $N$ implies $f_s/N$ decreases), which allows us to  a more dense range of discrete frequencies. Thanks to the similarity of the inverse DFT to the DFT, it is not difficult to modify our algorithm to work in the inverse direction. After we obtain our DFT data, we may modify it in a few ways. To implement an ideal filter, we can flatten the FFT of our signal with zeroes after (low-pass) or before (hi-pass) the cutoff frequency, ensuring we maintain the conjugate symmetry of the FFT. However, as previously mentioned, ideal filters will introduce un-wanted ringing via spectral leakage due to how we handled the drop-off in the frequency domain. As previously mentioned, this can be reduced with interpolation via a window function. For our use case, the mentioned Hanning window, defined: \[W_H=\frac{1}{2}(1-cos(2\pi\frac{n}{N})) \text{ when } 0\leq n=N-1, 0 \text{ otherwise}\] Which generates a nice bell-shaped curve that we can multiply across our frequency domain, making for a more gradual shift around the cutoff frequency which reduces spectral leakage. \cite{5}

\section{Results} Our first step to operating on signals is to convert them into a workable data type. We may use the pydub library to import .wav or .mp3 files as AudioSegment data types, which we may convert into numpy arrays for data handling and operation. Then, we can and calculate our FFTs on those arrays, outputting them as arrays. 


\begin{thebibliography}{9}
\bibitem{1} 
Bounchalen, Anna. \textit{An Elementary Introduction to Fast Fourier Transform Algorithms}. University of Chicago Mathematics REU, 2019. \url{https://math.uchicago.edu/~may/REU2019/REUPapers/Bounchaleun.pdf}

\bibitem{2}
\url{https://brianmcfee.net/dstbook-site/content/ch06-dft-properties/Conjugate-Symmetry.html}

\bibitem{3}
\url{https://www.analog.com/en/resources/glossary/frequency_bin.html}

\bibitem{4}
\url{https://www.dspguide.com/ch11/4.htm}

\bibitem{5}
\url{https://www.sciencedirect.com/topics/engineering/hanning-window}

\bibitem{6}
Keim, Robert. \textit{The Nyquist-Shannon Theorem: Understanding Sampled Systems - Technical Articles.}. All About Circuits, May 6 2020. \url{https://www.allaboutcircuits.com/technical-articles/nyquist-shannon-theorem-understanding-sampled-systems/}
\end{thebibliography}

\end{document}
